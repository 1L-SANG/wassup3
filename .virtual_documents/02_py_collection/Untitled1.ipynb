pip install selenium


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time


options = Options()
options.add_experimental_option("detach", True)


url = 'http://naver.com'
driver = webdriver.Chrome(options=options) # 객체 생성

driver.get(url) # 실행 

time.sleep(2)


driver.find_element(By.XPATH,'//*[@id="shortcutArea"]/ul/li[5]/a/span[1]')


driver.find_element(By.XPATH,'//*[@id="shortcutArea"]/ul/li[5]/a/span[1]').click()


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time, random, pandas as pd

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)

driver = webdriver.Chrome( options = options)
url = 'https://play.google.com/store/apps/details?id=com.estsoft.picnic'
driver.get(url)
time.sleep(random.randint(2,3))


driver.find_element(By.XPATH,'//*[@id="yDmH0d"]/c-wiz[2]/div/div/div[1]/div/div[2]/div/div[1]/div[1]/c-wiz[4]/section/div/div[2]/div[5]/div/div/button/span').click()


import time
import requests
from bs4 import BeautifulSoup
import os

url = "http://naver.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')


from bs4 import BeautifulSoup
import requests
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

PATH= 'https://naver.com/'
resp= requests.get(PATH)
src = resp.text
soup = BeautifulSoup(src, 'lxml') 

driver = webdriver.Chrome(options=options)

driver.get(PATH)


pages = soup.find_all('div', class_='page')
output_dir = "images"
os.makedirs(output_dir, exist_ok=True)

for page in pages:
    page_number = page.get('data-page-number')
    image_tag = page.find('img')  # img 태그를 찾습니다
    if image_tag and 'src' in image_tag.attrs:
        image_url = image_tag['src']
        image_response = requests.get(image_url)
        
        # 이미지를 파일로 저장
        with open(os.path.join(output_dir, f"page_{page_number}.jpg"), 'wb') as file:
            file.write(image_response.content)
        print(f"Saved page {page_number} image.")
    else:
        print(f"No image found for page {page_number}.")

print("Image extraction complete.")


from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


import os
f_dir = 'stock'
os.makedirs(os.getcwd()+'\\' + f_dir)

driver = webdriver.Chrome(options=options)
URL = 'http://data.krx.co.kr/contents/MDC/MDI/mdiLoader/index.cmd?menuId=MDC0301'  
driver.get(URL)


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


search = input('검색어:')
cnt = int(input('크롤링 할 건수는 몇건입니까?: '))
page_cnt = math.ceil(cnt / 10)  # 크롤링 할 전체 페이지 수


URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search
driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


driver.find_element(By.CSS_SELECTOR, "#s_attraction > .more_view > a").click()
